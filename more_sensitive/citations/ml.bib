@article{word2vec,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}

@article{fasttext,
  title={Enriching word vectors with subword information},
  author={Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  journal={Transactions of the Association for Computational Linguistics},
  volume={5},
  pages={135--146},
  year={2017},
  publisher={MIT Press}
}

@article{supervecx,
  title={Learning supervised embeddings for large scale sequence comparisons},
  author={Kimothi, Dhananjay and Biyani, Pravesh and Hogan, James M and Soni, Akshay and Kelly, Wayne},
  journal={PloS one},
  volume={15},
  number={3},
  pages={e0216636},
  year={2020},
  publisher={Public Library of Science San Francisco, CA USA}
}

@inproceedings{attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@article{elmo,
  title={Deep contextualized word representations},
  author={Peters, Matthew E and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1802.05365},
  year={2018}
}

@article{esm,
  title={Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences},
  author={Rives, Alexander and Goyal, Siddharth and Meier, Joshua and Guo, Demi and Ott, Myle and Zitnick, C Lawrence and Ma, Jerry and Fergus, Rob},
  journal={bioRxiv},
  pages={622803},
  year={2019},
  publisher={Cold Spring Harbor Laboratory}
}

@article{seqvec,
  title={Modeling aspects of the language of life through transfer-learning protein sequences},
  author={Heinzinger, Michael and Elnaggar, Ahmed and Wang, Yu and Dallago, Christian and Nechaev, Dmitrii and Matthes, Florian and Rost, Burkhard},
  journal={BMC bioinformatics},
  volume={20},
  number={1},
  pages={723},
  year={2019},
  publisher={Springer}
}

@inproceedings{tape,
  title={Evaluating protein transfer learning with TAPE},
  author={Rao, Roshan and Bhattacharya, Nicholas and Thomas, Neil and Duan, Yan and Chen, Peter and Canny, John and Abbeel, Pieter and Song, Yun},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9689--9701},
  year={2019}
}

@article{prottrans,
  title={ProtTrans: Towards Cracking the Language of Life's Code Through Self-Supervised Deep Learning and High Performance Computing},
  author={Elnaggar, Ahmed and Heinzinger, Michael and Dallago, Christian and Rihawi, Ghalia and Wang, Yu and Jones, Llion and Gibbs, Tom and Feher, Tamas and Angerer, Christoph and Bhowmik, Debsindhu and others},
  journal={arXiv preprint arXiv:2007.06225},
  year={2020}
}

@article{bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{albert,
  title={Albert: A lite bert for self-supervised learning of language representations},
  author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  journal={arXiv preprint arXiv:1909.11942},
  year={2019}
}

@article {cpcprot,
	author = {Lu, Amy X. and Zhang, Haoran and Ghassemi, Marzyeh and Moses, Alan},
	title = {Self-Supervised Contrastive Learning of Protein Representations By Mutual Information Maximization},
	elocation-id = {2020.09.04.283929},
	year = {2020},
	doi = {10.1101/2020.09.04.283929},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {Pretrained embedding representations of biological sequences which capture meaningful properties can alleviate many problems associated with supervised learning in biology. We apply the principle of mutual information maximization between local and global information as a self-supervised pretraining signal for protein embeddings. To do so, we divide protein sequences into fixed size fragments, and train an autoregressive model to distinguish between subsequent fragments from the same protein and fragments from random proteins. Our model, CPCProt, achieves comparable performance to state-of-the-art self-supervised models for protein sequence embeddings on various downstream tasks, but reduces the number of parameters down to 0.9\% to 8.9\% of benchmarked models. Further, we explore how downstream assessment protocols affect embedding evaluation, and the effect of contrastive learning hyperparameters on empirical performance. We hope that these results will inform the development of contrastive learning methods in protein biology and other modalities.Competing Interest StatementAmy X. Lu is current an employee of Insitro Inc. Work completed at the University of Toronto.},
	URL = {https://www.biorxiv.org/content/early/2020/09/06/2020.09.04.283929},
	eprint = {https://www.biorxiv.org/content/early/2020/09/06/2020.09.04.283929.full.pdf},
	journal = {bioRxiv}
}

@article{t5,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={arXiv preprint arXiv:1910.10683},
  year={2019}
}

@article{bio_embeddings,
  title={Learned embeddings from deep learning to visualize and predict protein sets},
  author={Dallago, Christian and Sch{\"u}tze, Konstantin and Heinzinger, Michael and Olenyi, Tobias and Littmann, Maria and Lu, Amy X and Yang, Kevin K and Min, Seonwoo and Yoon, Sungroh and Morton, James T and others},
  journal={Current Protocols},
  volume={1},
  number={5},
  pages={e113},
  year={2021},
  publisher={Wiley Online Library}
}

@article{signalp6,
  title={SignalP 6.0 achieves signal peptide prediction across all types using protein language models},
  author={Teufel, Felix and Armenteros, Jos{\'e} Juan Almagro and Johansen, Alexander Rosenberg and Gislason, Magn{\'u}s Halld{\'o}r and Pihl, Silas Irby and Tsirigos, Konstantinos D and Winther, Ole and Brunak, S{\o}ren and Von Heijne, Gunnar and Nielsen, Henrik},
  journal={bioRxiv},
  year={2021},
  publisher={Cold Spring Harbor Laboratory}
}

@article{light_attention,
  title={Light attention predicts protein location from the language of life},
  author={Staerk, Hannes and Dallago, Christian and Heinzinger, Michael and Rost, Burkhard},
  journal={bioRxiv},
  year={2021},
  publisher={Cold Spring Harbor Laboratory}
}

@article{transformer_structure,
  title={Transformer protein language models are unsupervised structure learners},
  author={Rao, Roshan and Ovchinnikov, Sergey and Meier, Joshua and Rives, Alexander and Sercu, Tom},
  journal={BioRxiv},
  year={2020},
  publisher={Cold Spring Harbor Laboratory}
}

@article{scaling_laws,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{alphafold2,
  title={Highly accurate protein structure prediction with AlphaFold},
  author={Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v{Z}}{\'i}dek, Augustin and Potapenko, Anna and others},
  journal={Nature},
  volume={596},
  number={7873},
  pages={583--589},
  year={2021},
  publisher={Nature Publishing Group}
}

@article{colabfold,
  title={ColabFold-Making protein folding accessible to all},
  author={Mirdita, Milot and Sch{\"u}tze, Konstantin and Moriwaki, Yoshitaka and Heo, Lim and Ovchinnikov, Sergey and Steinegger, Martin},
  year={2021}
}
